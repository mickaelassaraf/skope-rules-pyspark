{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.8/site-packages (25.0.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (75.3.2)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.8/site-packages (0.45.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.venv/lib/python3.8/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.8/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib/python3.8/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.venv/lib/python3.8/site-packages (from seaborn) (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.8/site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./.venv/lib/python3.8/site-packages (from pyarrow) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.23.5\n",
      "Uninstalling numpy-1.23.5:\n",
      "  Successfully uninstalled numpy-1.23.5\n",
      "Found existing installation: scikit-learn 1.1.3\n",
      "Uninstalling scikit-learn-1.1.3:\n",
      "  Successfully uninstalled scikit-learn-1.1.3\n",
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting scikit-learn==1.1.3\n",
      "  Using cached scikit_learn-1.1.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.3) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.3) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.3) (3.5.0)\n",
      "Using cached numpy-1.23.5-cp38-cp38-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "Using cached scikit_learn-1.1.3-cp38-cp38-macosx_10_9_x86_64.whl (8.6 MB)\n",
      "Installing collected packages: numpy, scikit-learn\n",
      "Successfully installed numpy-1.23.5 scikit-learn-1.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy scikit-learn -y\n",
    "!pip install numpy==1.23.5 scikit-learn==1.1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.venv/lib/python3.8/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.8/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib/python3.8/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.venv/lib/python3.8/site-packages (from seaborn) (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.8/site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./.venv/lib/python3.8/site-packages (from pyarrow) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark==3.3.1 scikit-learn==1.1.3 cloudpickle==2.2.1 numpy==1.23.5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.23.5\n",
      "Uninstalling numpy-1.23.5:\n",
      "  Successfully uninstalled numpy-1.23.5\n",
      "Found existing installation: scikit-learn 1.1.3\n",
      "Uninstalling scikit-learn-1.1.3:\n",
      "  Successfully uninstalled scikit-learn-1.1.3\n",
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting scikit-learn==1.1.3\n",
      "  Using cached scikit_learn-1.1.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.3) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.3) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn==1.1.3) (3.5.0)\n",
      "Using cached numpy-1.23.5-cp38-cp38-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "Using cached scikit_learn-1.1.3-cp38-cp38-macosx_10_9_x86_64.whl (8.6 MB)\n",
      "Installing collected packages: numpy, scikit-learn\n",
      "Successfully installed numpy-1.23.5 scikit-learn-1.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy scikit-learn -y\n",
    "!pip install numpy==1.23.5 scikit-learn==1.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skoperulespyspark.skope_rules import SkopeRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_0  feature_1  feature_2  feature_3  feature_4\n",
      "0   0.621810  -1.651952  -1.570225  -1.442228   2.085088\n",
      "1  -0.704344  -1.799831  -1.408461  -0.965512   1.906484\n",
      "2  -0.095296   0.481188   0.279022  -0.934015   1.924062\n",
      "3  -0.241236  -0.728721   0.352055  -0.467196   0.131468\n",
      "4  -1.280304   0.708512   0.872457   0.887858  -0.697708\n",
      "[0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Création d'un dataset de classification binaire simple\n",
    "X, y = make_classification(\n",
    "    n_samples=200,     # nombre d'échantillons\n",
    "    n_features=5,      # nombre de features\n",
    "    n_informative=3,   # nombre de features vraiment utiles\n",
    "    n_redundant=0,     # pas de features redondants\n",
    "    n_clusters_per_class=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Optionnel : transformer X en DataFrame pour y mettre des noms de colonnes\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Affichage de la forme des données\n",
    "print(X_df.head())\n",
    "print(y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/02 16:50:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SkopeRulesData\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.5.3\n",
      "  Downloading pandas-1.5.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.5.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas==1.5.3) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.venv/lib/python3.8/site-packages (from pandas==1.5.3) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
      "Downloading pandas-1.5.3-cp38-cp38-macosx_10_9_x86_64.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "Successfully installed pandas-1.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas==1.5.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mickaelassaraf/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/mickaelassaraf/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|           feature_0|          feature_1|          feature_2|          feature_3|          feature_4|\n",
      "+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  0.6218099622171964|-1.6519521220167126|-1.5702247198904564|-1.4422277399897183|  2.085088172014556|\n",
      "| -0.7043436905427592|-1.7998314306629748|-1.4084612963635597|-0.9655118629004646| 1.9064841686885794|\n",
      "| -0.0952955323869521|0.48118826022148664|0.27902152577033923|   -0.9340147424513|   1.92406212449343|\n",
      "|-0.24123605785632987|-0.7287209723619907| 0.3520553965142968|-0.4671955130386416| 0.1314683783130567|\n",
      "| -1.2803043986708944| 0.7085117299020361| 0.8724573282801447| 0.8878576729295498|-0.6977081418283249|\n",
      "+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark = spark.createDataFrame(X_df)\n",
    "df_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkopeRules(\n",
    "    numTrees=100,  # nombre d'arbres\n",
    "    max_depth=5,       # profondeur maximale des arbres\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Règles apprises :\n",
      "('__C__1 > -0.1953609362244606 and __C__4 > -0.6533460021018982', (1.0, 0.9523809523809523, 2))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 > -1.2627553343772888', (0.8947368421052632, 1.0, 2))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 > -0.6219995021820068', (1.0, 0.888198757763975, 10))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 > -0.557672306895256', (1.0, 0.8871635610766045, 4))\n",
      "('__C__1 > -0.1929747611284256 and __C__4 > -0.6533460021018982', (1.0, 0.8450079744816587, 10))\n",
      "('__C__1 > -0.19522278755903244 and __C__4 > -0.6533460021018982', (1.0, 0.8321905837433787, 14))\n",
      "('__C__1 > -0.20200984925031662 and __C__4 > -0.6533460021018982', (1.0, 0.829125467536713, 14))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 > -0.7647395431995392', (0.9370129870129871, 0.8753122197495599, 14))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 > -0.6533460021018982', (1.0, 0.8205140394353564, 82))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 > -1.1677517890930176', (0.8973684210526316, 0.8973684210526316, 4))\n",
      "('__C__0 <= 2.543508529663086 and __C__1 > -0.20200984925031662', (0.85, 0.9444444444444444, 2))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 > -0.7960860431194305', (0.9445728291316525, 0.8351178152420389, 14))\n",
      "('__C__1 > -0.17535144090652466 and __C__4 > -0.6219995021820068', (1.0, 0.7916666666666666, 2))\n",
      "('__C__1 > -0.19760896265506744 and __C__4 > -0.6219995021820068', (1.0, 0.782608695652174, 2))\n",
      "('__C__0 > -1.4679741859436035 and __C__1 > -0.3302100747823715 and __C__4 > -0.6629402041435242', (0.8235294117647058, 0.9333333333333333, 2))\n",
      "('__C__1 > -0.26571063697338104 and __C__4 > -0.6533460021018982', (0.9444444444444444, 0.8107503607503607, 6))\n",
      "('__C__1 > -0.1953609362244606 and __C__4 > -0.6219995021820068', (1.0, 0.7619047619047619, 2))\n",
      "('__C__1 > -0.26571063697338104 and __C__4 > -0.6219995021820068', (0.9411764705882353, 0.8, 2))\n",
      "('__C__1 > -0.19522278755903244 and __C__4 > -1.1677517890930176', (0.9047619047619048, 0.8260869565217391, 2))\n",
      "('__C__1 > -0.26806534826755524 and __C__4 > -0.7960860431194305', (0.85, 0.85, 2))\n",
      "('__C__1 > -0.19522278755903244 and __C__4 > -0.7960860431194305', (0.9375, 0.75, 2))\n",
      "('__C__1 > -0.2286025509238243 and __C__4 > -0.6533460021018982', (0.9090909090909091, 0.7692307692307693, 2))\n",
      "('__C__1 > -0.26571063697338104 and __C__4 > -0.7960860431194305', (0.8333333333333334, 0.7894736842105263, 2))\n",
      "('__C__1 > -0.20200984925031662 and __C__4 > -0.6219995021820068', (1.0, 0.6666666666666666, 2))\n",
      "('__C__1 <= -0.3302100747823715 and __C__2 <= 2.133874297142029 and __C__3 <= 0.22767794132232666 and __C__4 <= 1.3310196995735168', (0.5, 0.22727272727272727, 2))\n",
      "('__C__1 <= -0.19962367415428162 and __C__1 > -0.8470495045185089 and __C__3 <= 0.22767794132232666 and __C__4 > 0.89145627617836', (0.6666666666666666, 0.125, 2))\n",
      "('__C__1 <= -0.1929747611284256 and __C__3 > 0.19318214058876038 and __C__4 > -1.0028285384178162', (0.6666666666666666, 0.1, 2))\n",
      "('__C__1 <= -0.19962367415428162 and __C__2 <= 2.133874297142029 and __C__3 > 0.22767794132232666 and __C__4 > -1.0028285384178162', (1.0, 0.09523809523809523, 2))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 <= -0.7647395431995392', (0.5, 0.09090909090909091, 2))\n",
      "('__C__1 <= -0.9879131615161896 and __C__3 <= 0.22767794132232666 and __C__4 <= 1.05271714925766 and __C__4 > -1.0028285384178162', (0.5, 0.09090909090909091, 2))\n",
      "('__C__1 <= -0.19962367415428162 and __C__3 <= -0.932559996843338 and __C__4 <= 0.89145627617836 and __C__4 > -1.0028285384178162', (0.6666666666666666, 0.08695652173913043, 2))\n",
      "('__C__1 > -0.19522278755903244 and __C__4 <= -0.6533460021018982', (0.5, 0.06904761904761905, 4))\n",
      "('__C__1 > -0.19962367415428162 and __C__4 <= -0.6533460021018982', (0.5, 0.05488906088751291, 10))\n",
      "('__C__1 <= -0.19962367415428162 and __C__3 > 0.19318214058876038', (0.5, 0.05385964912280702, 10))\n",
      "('__C__1 <= -0.19962367415428162 and __C__3 <= -1.4237951636314392 and __C__4 <= 1.3310196995735168 and __C__4 > -1.0028285384178162', (0.5, 0.05, 2))\n",
      "('__C__1 <= -0.26571063697338104 and __C__3 <= -0.9558891654014587 and __C__4 <= 0.8561038970947266', (0.5, 0.05, 2))\n",
      "('__C__1 <= 1.0104527473449707 and __C__1 > -0.19962367415428162 and __C__4 <= -0.557672306895256', (0.5, 0.047619047619047616, 2))\n",
      "('__C__1 <= -1.0469942688941956 and __C__2 <= 0.2750926800072193 and __C__3 <= -0.12716330215334892 and __C__4 <= 1.05271714925766', (0.5, 0.047619047619047616, 2))\n",
      "('__C__1 <= -0.8470495045185089 and __C__1 > -1.3942845463752747 and __C__3 <= 0.19318214058876038 and __C__3 > -0.717133492231369', (0.5, 0.047619047619047616, 2))\n",
      "('__C__1 <= -0.19962367415428162 and __C__1 > -0.9513200223445892 and __C__3 <= 0.22767794132232666 and __C__4 > 1.317829966545105', (0.5, 0.045454545454545456, 2))\n",
      "('__C__1 <= -0.1929747611284256 and __C__1 > -0.8712345063686371 and __C__4 > 1.002695471048355', (0.5, 0.045454545454545456, 2))\n",
      "('__C__1 > -0.19760896265506744 and __C__4 <= -0.6219995021820068', (0.5, 0.043478260869565216, 2))\n",
      "('__C__1 <= -0.19522278755903244 and __C__3 > 0.19318214058876038', (0.5, 0.043478260869565216, 2))\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_df, y)\n",
    "# Affichage des règles apprises\n",
    "print(\"Règles apprises :\")\n",
    "for rule in model.rules_:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.1.3 in ./.venv/lib/python3.9/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.9/site-packages (from scikit-learn==1.1.3) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.venv/lib/python3.9/site-packages (from scikit-learn==1.1.3) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn==1.1.3) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn==1.1.3) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.param.shared import Param\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import countDistinct, col, when, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from warnings import warn\n",
    "from collections.abc import Iterable \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# ======================\n",
    "# Estimator (Trainer)\n",
    "# ======================\n",
    "class SkopeRulesPySpark(Estimator, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    def __init__(self,\n",
    "                 feature_names=None,\n",
    "                 precision_min=0.5,\n",
    "                 recall_min=0.01,\n",
    "                 numTrees=10,\n",
    "                 subsamplingRate=0.8,\n",
    "                 featureSubsetStrategy='all',\n",
    "                 bootstrap=False,\n",
    "                 max_Depth=3,\n",
    "                 max_depth_duplication=None,\n",
    "                 minInstancesPerNode=2,\n",
    "                 n_jobs=1,\n",
    "                 seed=None,\n",
    "                 verbose=0):\n",
    "\n",
    "        super(SkopeRulesPySpark, self).__init__()\n",
    "\n",
    "        # Déclaration des Param\n",
    "        self.feature_names = Param(self, \"feature_names\", \"List of feature names\")\n",
    "        self.precision_min = Param(self, \"precision_min\", \"Minimum precision\")\n",
    "        self.recall_min = Param(self, \"recall_min\", \"Minimum recall\")\n",
    "        self.numTrees = Param(self, \"numTrees\", \"Number of estimators\")\n",
    "        self.subsamplingRate = Param(self, \"subsamplingRate\", \"subsamplingRate\")\n",
    "        self.featureSubsetStrategy = Param(self, \"featureSubsetStrategy\", \"featureSubsetStrategy\")\n",
    "        self.bootstrap = Param(self, \"bootstrap\", \"Bootstrap samples\")\n",
    "        self.max_Depth = Param(self, \"max_depth\", \"Max tree depth\")\n",
    "        self.max_depth_duplication = Param(self, \"max_depth_duplication\", \"Max depth duplication\")\n",
    "        self.minInstancesPerNode = Param(self, \"minInstancesPerNode\", \"Min samples split\")\n",
    "        self.n_jobs = Param(self, \"n_jobs\", \"Number of jobs\")\n",
    "        self.seed = Param(self, \"seed\", \"Random state\")\n",
    "        self.verbose = Param(self, \"verbose\", \"Verbosity level\")\n",
    "\n",
    "        # Définir les valeurs par défaut\n",
    "        self._setDefault(\n",
    "            feature_names=None,\n",
    "            precision_min=0.5,\n",
    "            recall_min=0.01,\n",
    "            numTrees=10,\n",
    "            subsamplingRate=0.8,\n",
    "            featureSubsetStrategy='all',\n",
    "            bootstrap=False,\n",
    "            max_Depth=3,\n",
    "            max_depth_duplication=None,\n",
    "            minInstancesPerNode=2,\n",
    "            n_jobs=1,\n",
    "            seed=None,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Appliquer les paramètres\n",
    "        kwargs = {\n",
    "            \"feature_names\": feature_names,\n",
    "            \"precision_min\": precision_min,\n",
    "            \"recall_min\": recall_min,\n",
    "            \"numTrees\": numTrees,\n",
    "            \"subsamplingRate\": subsamplingRate,\n",
    "            \"featureSubsetStrategy\":featureSubsetStrategy,\n",
    "            \"bootstrap\": bootstrap,\n",
    "            \"max_Depth\": max_Depth,\n",
    "            \"max_depth_duplication\": max_depth_duplication,\n",
    "            \"minInstancesPerNode\": minInstancesPerNode,\n",
    "            \"n_jobs\": n_jobs,\n",
    "            \"seed\": seed,\n",
    "            \"verbose\": verbose\n",
    "        }\n",
    "\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    def setParams(self, **kwargs):\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _fit(self, dataset: DataFrame):\n",
    "        label_col = \"label\"\n",
    "        features_col = \"features\"\n",
    "\n",
    "        classes = dataset.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "        self.classes_ = classes\n",
    "        n_classes = len(classes)\n",
    "\n",
    "        if n_classes < 2:\n",
    "            first_class = dataset.select(label_col).first()[0]\n",
    "            raise ValueError(\n",
    "                f\"This method needs samples of at least 2 classes, but the data contains only one class: {first_class}\"\n",
    "            )\n",
    "        if set(classes) != {0, 1}:\n",
    "            warn(f\"Found labels {classes}. This method assumes target class to be labeled as 1 and normal data as 0.\")\n",
    "            dataset = dataset.withColumn(label_col, when(col(label_col) != 0, lit(1)).otherwise(lit(0)))\n",
    "\n",
    "\n",
    "            # Count total number of rows\n",
    "        n_samples = dataset.count()\n",
    "\n",
    "        subsamplingRate_raw = self.getOrDefault(\"subsamplingRate\")\n",
    "\n",
    "        # Vérif type\n",
    "        if isinstance(subsamplingRate_raw, str):\n",
    "            raise ValueError(f'subsamplingRate (\"{subsamplingRate_raw}\") is not supported. Use float or int.')\n",
    "\n",
    "        elif isinstance(subsamplingRate_raw, int):\n",
    "            if subsamplingRate_raw > n_samples:\n",
    "                warn(f\"subsamplingRate ({subsamplingRate_raw}) is greater than total samples ({n_samples}). It will be set to {n_samples}.\")\n",
    "                subsamplingRate = n_samples\n",
    "            else:\n",
    "                subsamplingRate = subsamplingRate_raw\n",
    "\n",
    "        elif isinstance(subsamplingRate_raw, float):\n",
    "            if not (0. < subsamplingRate_raw <= 1.):\n",
    "                raise ValueError(f\"subsamplingRate must be in (0, 1], got {subsamplingRate_raw}\")\n",
    "            subsamplingRate = int(subsamplingRate_raw * n_samples)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for subsamplingRate: {type(subsamplingRate_raw)}\")\n",
    "\n",
    "        # Stocker l’info dans le modèle plus tard si besoin\n",
    "        self.subsamplingRate_ = subsamplingRate\n",
    "\n",
    "        self.rules_ = {}\n",
    "        self.estimators_ = []\n",
    "        self.estimators_samples_ = []\n",
    "        self.estimators_features_ = []\n",
    "\n",
    "        if feature_names is None:\n",
    "            ValueError(\"feature_names must be provided\")\n",
    "\n",
    "        clfs = []\n",
    "        regs = []\n",
    "\n",
    "        max_depth_val = self.getOrDefault(\"max_Depth\")\n",
    "        if isinstance(max_depth_val, Iterable) and not isinstance(max_depth_val, str):\n",
    "            self._max_depths = list(max_depth_val)\n",
    "        else:\n",
    "            self._max_depths = [max_depth_val]\n",
    "    \n",
    "\n",
    "        for max_Depth in self._max_depths:\n",
    "\n",
    "            \n",
    "            bagging_clf = RandomForestClassifier(minInstancesPerNode =self.minInstancesPerNode, \n",
    "                                                 numTrees=self.numTrees,\n",
    "                                                 subsamplingRate=subsamplingRate, \n",
    "                                                 maxDepth=max_Depth, \n",
    "                                                 seed=self.seed,\n",
    "                                                 bootstrap=self.bootstrap,\n",
    "                                                 featureSubsetStrategy=self.featureSubsetStrategy)\n",
    "            \n",
    "            bagging_reg = RandomForestRegressor(minInstancesPerNode =self.minInstancesPerNode, \n",
    "                                                 numTrees=self.numTrees,\n",
    "                                                 subsamplingRate=subsamplingRate, \n",
    "                                                 maxDepth=max_Depth, \n",
    "                                                 seed=self.seed,\n",
    "                                                 bootstrap=self.bootstrap,\n",
    "                                                 featureSubsetStrategy=self.featureSubsetStrategy)\n",
    "            clfs.append(bagging_clf)\n",
    "            regs.append(bagging_reg)\n",
    "\n",
    "            # NOT IMPLEMENTED: \n",
    "        #     if sample_weight is not None:\n",
    "        #     if sample_weight is not None:\n",
    "        #         sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "        #     weights = sample_weight - sample_weight.min()\n",
    "        #     contamination = float(sum(y)) / len(y)\n",
    "        #     y_reg = (\n",
    "        #         pow(weights, 0.5) * 0.5 / contamination * (y > 0) -\n",
    "        #         pow((weights).mean(), 0.5) * (y == 0))\n",
    "        #     y_reg = 1. / (1 + np.exp(-y_reg))  # sigmoid\n",
    "        # else:\n",
    "        #     y_reg = y  # same as an other classification bagging\n",
    "\n",
    "\n",
    "            for clf in clfs:\n",
    "                clf.fit(dataset)\n",
    "                self.estimators_.append(clf)\n",
    "\n",
    "       \n",
    "       \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Training done. Returning model...\")\n",
    "        return SkopeRulesModelPySpark()\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Model (Predictor)\n",
    "# ======================\n",
    "class SkopeRulesModelPySpark(Model, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        print(\"Applying model (no-op)...\")\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mickaelassaraf/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/mickaelassaraf/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done. Returning model...\n"
     ]
    }
   ],
   "source": [
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "y_df = pd.DataFrame(y, columns=[\"label\"])\n",
    "df = pd.concat([X_df, y_df], axis=1)\n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "model = SkopeRulesPySpark()\n",
    "pipeline = Pipeline(stages=[\n",
    "    VectorAssembler(inputCols=feature_names, outputCol=\"features\"),\n",
    "    model(feature_names)\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
       "                  bootstrap=False, max_samples=0.8, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
       "                  bootstrap=False, max_samples=0.8, n_jobs=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
       "                  bootstrap=False, max_samples=0.8, n_jobs=-1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(\n",
    "        max_depth=3,\n",
    "        min_samples_split=2,\n",
    "        seed=None\n",
    "    ),\n",
    "    n_estimators=10,\n",
    "    max_samples=0.8,\n",
    "    max_features=1.0,\n",
    "    bootstrap=False,\n",
    "    bootstrap_features=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(minInstancesPerNode =2, numTrees=10,subsamplingRate=0.8, maxDepth=3,seed=None,bootstrap=False,featureSubsetStrategy=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mickaelassaraf/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/mickaelassaraf/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "y_df = pd.DataFrame(y, columns=[\"label\"])\n",
    "df = pd.concat([X_df, y_df], axis=1)\n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "model = SkopeRulesPySpark()\n",
    "pipeline = Pipeline(stages=[\n",
    "    VectorAssembler(inputCols=feature_names, outputCol=\"features\"),\n",
    "    RandomForestClassifier(minInstancesPerNode =2, numTrees=10,subsamplingRate=0.8, maxDepth=3,seed=None,bootstrap=False,featureSubsetStrategy=\"all\")\n",
    "    ])\n",
    "\n",
    "pipeline_model = pipeline.fit(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassificationModel: uid=dtc_e4e97674bd18, depth=3, numNodes=11, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_3fe2bc82093d, depth=3, numNodes=9, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_0af94c692544, depth=2, numNodes=7, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_5713a5de7155, depth=3, numNodes=9, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_36a8b6965402, depth=3, numNodes=9, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_52c3ea292c5f, depth=3, numNodes=9, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_8881bb6db144, depth=3, numNodes=11, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_3da2c79205bf, depth=2, numNodes=7, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_5c41561474c1, depth=3, numNodes=9, numClasses=2, numFeatures=5,\n",
       " DecisionTreeClassificationModel: uid=dtc_5a2c96198b2f, depth=3, numNodes=11, numClasses=2, numFeatures=5]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.stages[1].trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.sql import DataFrame\n",
    "import random\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class BaggingEstimator(Estimator):\n",
    "    def __init__(self, base_estimator, n_estimators=10, max_samples=0.8, max_features=1.0, bootstrap=True, n_jobs=1, random_state=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def _fit(self, dataset: DataFrame):\n",
    "        # Initialiser les modèles de base\n",
    "        models = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # Échantillonnage bootstrap des données\n",
    "            sampled_data = self._bootstrap_sample(dataset)\n",
    "            print(\"base_estimator\", self.base_estimator)\n",
    "            # Entraîner chaque modèle sur l'échantillon\n",
    "            model = self.base_estimator.fit(sampled_data)\n",
    "            models.append(model)\n",
    "        \n",
    "        # Retourner un modèle Bagging qui contient tous les modèles de base\n",
    "        return BaggingModel(models)\n",
    "    \n",
    "    def _bootstrap_sample(self, dataset: DataFrame):\n",
    "        \"\"\"\n",
    "        Effectue un échantillonnage bootstrap des données.\n",
    "        Si self.bootstrap est True, l'échantillon est effectué avec remplacement.\n",
    "        \"\"\"\n",
    "        n_samples = dataset.count()\n",
    "        sample_size = int(self.max_samples * n_samples) if self.bootstrap else n_samples\n",
    "        \n",
    "        # Générer des indices aléatoires pour l'échantillonnage bootstrap\n",
    "        sampled_indices = [random.randint(0, n_samples - 1) for _ in range(sample_size)]\n",
    "        \n",
    "        # Sélectionner les lignes correspondantes aux indices\n",
    "        sampled_data = dataset.rdd.zipWithIndex().filter(lambda x: x[1] in sampled_indices).map(lambda x: x[0]).toDF()\n",
    "        \n",
    "        return sampled_data\n",
    "\n",
    "\n",
    "from pyspark.ml import Model\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class BaggingModel(Model):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        # Get predictions from all models\n",
    "        predictions = [model.transform(dataset).select(\"unique_id\", F.col(\"prediction\").alias(f\"prediction_{i}\")) for i, model in enumerate(self.models)]\n",
    "        \n",
    "        # Merge predictions on unique_id\n",
    "        print(\"predictions\", predictions)\n",
    "        pred_df = predictions[0]\n",
    "        for df in predictions[1:]:\n",
    "            pred_df = pred_df.join(df, on=\"unique_id\", how=\"left\")\n",
    "\n",
    "        # Compute final prediction (majority vote)\n",
    "        cols = [F.col(f\"prediction_{i}\") for i in range(len(self.models))]\n",
    "        final_prediction = pred_df.withColumn(\"final_prediction\", F.expr(f\"array({', '.join([f'prediction_{i}' for i in range(len(self.models))])})\")) \\\n",
    "                                  .withColumn(\"final_prediction\", F.expr(\"aggregate(final_prediction, 0, (acc, x) -> acc + x) / {} > 0.5\".format(len(self.models)))) \\\n",
    "                                  .withColumn(\"final_prediction\", F.col(\"final_prediction\").cast(\"int\"))\n",
    "\n",
    "        # Select final output\n",
    "        return final_prediction.select(\"unique_id\", \"final_prediction\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class AddIDTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self):\n",
    "        super(AddIDTransformer, self).__init__()\n",
    "\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        return dataset.withColumn(\"unique_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n",
      "base_estimator DecisionTreeClassifier_ca5d51bb0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f7b00a35c10>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mickaelassaraf/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 53, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'DecisionTreeClassifier' object has no attribute '_java_obj'\n",
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f7b00a35c10>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mickaelassaraf/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 53, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'DecisionTreeClassifier' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "estimator = BaggingEstimator(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    max_samples=0.8,\n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=None\n",
    ")\n",
    "add_id = AddIDTransformer()\n",
    "pipeline = Pipeline(stages=[\n",
    "    add_id,\n",
    "    VectorAssembler(inputCols=feature_names, outputCol=\"features\"),\n",
    "    estimator\n",
    "    ])\n",
    "\n",
    "pipeline_model = pipeline.fit(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions [DataFrame[unique_id: bigint, prediction_0: double], DataFrame[unique_id: bigint, prediction_1: double], DataFrame[unique_id: bigint, prediction_2: double], DataFrame[unique_id: bigint, prediction_3: double], DataFrame[unique_id: bigint, prediction_4: double], DataFrame[unique_id: bigint, prediction_5: double], DataFrame[unique_id: bigint, prediction_6: double], DataFrame[unique_id: bigint, prediction_7: double], DataFrame[unique_id: bigint, prediction_8: double], DataFrame[unique_id: bigint, prediction_9: double]]\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'aggregate(final_prediction, 0, lambdafunction((CAST(namedlambdavariable() AS DOUBLE) + namedlambdavariable()), namedlambdavariable(), namedlambdavariable()), lambdafunction(namedlambdavariable(), namedlambdavariable()))' due to data type mismatch: argument 3 requires int type, however, 'lambdafunction((CAST(namedlambdavariable() AS DOUBLE) + namedlambdavariable()), namedlambdavariable(), namedlambdavariable())' is of double type.;\nProject [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677, prediction_9#11738, ((cast(aggregate(final_prediction#11921, 0, lambdafunction((cast(lambda acc#11935 as double) + lambda x#11936), lambda acc#11935, lambda x#11936, false), lambdafunction(lambda id#11937, lambda id#11937, false)) as double) / cast(10 as double)) > cast(0.5 as double)) AS final_prediction#11934]\n+- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677, prediction_9#11738, array(prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677, prediction_9#11738) AS final_prediction#11921]\n   +- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677, prediction_9#11738]\n      +- Join LeftOuter, (unique_id#11113L = unique_id#11903L)\n         :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677]\n         :  +- Join LeftOuter, (unique_id#11113L = unique_id#11880L)\n         :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616]\n         :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11858L)\n         :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555]\n         :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11837L)\n         :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494]\n         :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11817L)\n         :     :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433]\n         :     :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11798L)\n         :     :     :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372]\n         :     :     :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11780L)\n         :     :     :     :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311]\n         :     :     :     :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11763L)\n         :     :     :     :     :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250]\n         :     :     :     :     :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11747L)\n         :     :     :     :     :     :     :     :     :- Project [unique_id#11113L, prediction#11163 AS prediction_0#11189]\n         :     :     :     :     :     :     :     :     :  +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, unique_id#11113L, features#11122, rawPrediction#11133, probability#11146, UDF(rawPrediction#11133) AS prediction#11163]\n         :     :     :     :     :     :     :     :     :     +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, unique_id#11113L, features#11122, rawPrediction#11133, UDF(rawPrediction#11133) AS probability#11146]\n         :     :     :     :     :     :     :     :     :        +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, unique_id#11113L, features#11122, UDF(features#11122) AS rawPrediction#11133]\n         :     :     :     :     :     :     :     :     :           +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, unique_id#11113L, UDF(struct(feature_0, feature_0#184, feature_1, feature_1#185, feature_2, feature_2#186, feature_3, feature_3#187, feature_4, feature_4#188)) AS features#11122]\n         :     :     :     :     :     :     :     :     :              +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, monotonically_increasing_id() AS unique_id#11113L]\n         :     :     :     :     :     :     :     :     :                 +- LogicalRDD [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L], false\n         :     :     :     :     :     :     :     :     +- Project [unique_id#11747L, prediction#11224 AS prediction_1#11250]\n         :     :     :     :     :     :     :     :        +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, unique_id#11747L, features#11122, rawPrediction#11194, probability#11207, UDF(rawPrediction#11194) AS prediction#11224]\n         :     :     :     :     :     :     :     :           +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, unique_id#11747L, features#11122, rawPrediction#11194, UDF(rawPrediction#11194) AS probability#11207]\n         :     :     :     :     :     :     :     :              +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, unique_id#11747L, features#11122, UDF(features#11122) AS rawPrediction#11194]\n         :     :     :     :     :     :     :     :                 +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, unique_id#11747L, UDF(struct(feature_0, feature_0#11741, feature_1, feature_1#11742, feature_2, feature_2#11743, feature_3, feature_3#11744, feature_4, feature_4#11745)) AS features#11122]\n         :     :     :     :     :     :     :     :                    +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, monotonically_increasing_id() AS unique_id#11747L]\n         :     :     :     :     :     :     :     :                       +- LogicalRDD [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L], false\n         :     :     :     :     :     :     :     +- Project [unique_id#11763L, prediction#11285 AS prediction_2#11311]\n         :     :     :     :     :     :     :        +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, unique_id#11763L, features#11122, rawPrediction#11255, probability#11268, UDF(rawPrediction#11255) AS prediction#11285]\n         :     :     :     :     :     :     :           +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, unique_id#11763L, features#11122, rawPrediction#11255, UDF(rawPrediction#11255) AS probability#11268]\n         :     :     :     :     :     :     :              +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, unique_id#11763L, features#11122, UDF(features#11122) AS rawPrediction#11255]\n         :     :     :     :     :     :     :                 +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, unique_id#11763L, UDF(struct(feature_0, feature_0#11757, feature_1, feature_1#11758, feature_2, feature_2#11759, feature_3, feature_3#11760, feature_4, feature_4#11761)) AS features#11122]\n         :     :     :     :     :     :     :                    +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, monotonically_increasing_id() AS unique_id#11763L]\n         :     :     :     :     :     :     :                       +- LogicalRDD [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L], false\n         :     :     :     :     :     :     +- Project [unique_id#11780L, prediction#11346 AS prediction_3#11372]\n         :     :     :     :     :     :        +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, unique_id#11780L, features#11122, rawPrediction#11316, probability#11329, UDF(rawPrediction#11316) AS prediction#11346]\n         :     :     :     :     :     :           +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, unique_id#11780L, features#11122, rawPrediction#11316, UDF(rawPrediction#11316) AS probability#11329]\n         :     :     :     :     :     :              +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, unique_id#11780L, features#11122, UDF(features#11122) AS rawPrediction#11316]\n         :     :     :     :     :     :                 +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, unique_id#11780L, UDF(struct(feature_0, feature_0#11774, feature_1, feature_1#11775, feature_2, feature_2#11776, feature_3, feature_3#11777, feature_4, feature_4#11778)) AS features#11122]\n         :     :     :     :     :     :                    +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, monotonically_increasing_id() AS unique_id#11780L]\n         :     :     :     :     :     :                       +- LogicalRDD [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L], false\n         :     :     :     :     :     +- Project [unique_id#11798L, prediction#11407 AS prediction_4#11433]\n         :     :     :     :     :        +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, unique_id#11798L, features#11122, rawPrediction#11377, probability#11390, UDF(rawPrediction#11377) AS prediction#11407]\n         :     :     :     :     :           +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, unique_id#11798L, features#11122, rawPrediction#11377, UDF(rawPrediction#11377) AS probability#11390]\n         :     :     :     :     :              +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, unique_id#11798L, features#11122, UDF(features#11122) AS rawPrediction#11377]\n         :     :     :     :     :                 +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, unique_id#11798L, UDF(struct(feature_0, feature_0#11792, feature_1, feature_1#11793, feature_2, feature_2#11794, feature_3, feature_3#11795, feature_4, feature_4#11796)) AS features#11122]\n         :     :     :     :     :                    +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, monotonically_increasing_id() AS unique_id#11798L]\n         :     :     :     :     :                       +- LogicalRDD [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L], false\n         :     :     :     :     +- Project [unique_id#11817L, prediction#11468 AS prediction_5#11494]\n         :     :     :     :        +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, unique_id#11817L, features#11122, rawPrediction#11438, probability#11451, UDF(rawPrediction#11438) AS prediction#11468]\n         :     :     :     :           +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, unique_id#11817L, features#11122, rawPrediction#11438, UDF(rawPrediction#11438) AS probability#11451]\n         :     :     :     :              +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, unique_id#11817L, features#11122, UDF(features#11122) AS rawPrediction#11438]\n         :     :     :     :                 +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, unique_id#11817L, UDF(struct(feature_0, feature_0#11811, feature_1, feature_1#11812, feature_2, feature_2#11813, feature_3, feature_3#11814, feature_4, feature_4#11815)) AS features#11122]\n         :     :     :     :                    +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, monotonically_increasing_id() AS unique_id#11817L]\n         :     :     :     :                       +- LogicalRDD [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L], false\n         :     :     :     +- Project [unique_id#11837L, prediction#11529 AS prediction_6#11555]\n         :     :     :        +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, unique_id#11837L, features#11122, rawPrediction#11499, probability#11512, UDF(rawPrediction#11499) AS prediction#11529]\n         :     :     :           +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, unique_id#11837L, features#11122, rawPrediction#11499, UDF(rawPrediction#11499) AS probability#11512]\n         :     :     :              +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, unique_id#11837L, features#11122, UDF(features#11122) AS rawPrediction#11499]\n         :     :     :                 +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, unique_id#11837L, UDF(struct(feature_0, feature_0#11831, feature_1, feature_1#11832, feature_2, feature_2#11833, feature_3, feature_3#11834, feature_4, feature_4#11835)) AS features#11122]\n         :     :     :                    +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, monotonically_increasing_id() AS unique_id#11837L]\n         :     :     :                       +- LogicalRDD [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L], false\n         :     :     +- Project [unique_id#11858L, prediction#11590 AS prediction_7#11616]\n         :     :        +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, unique_id#11858L, features#11122, rawPrediction#11560, probability#11573, UDF(rawPrediction#11560) AS prediction#11590]\n         :     :           +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, unique_id#11858L, features#11122, rawPrediction#11560, UDF(rawPrediction#11560) AS probability#11573]\n         :     :              +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, unique_id#11858L, features#11122, UDF(features#11122) AS rawPrediction#11560]\n         :     :                 +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, unique_id#11858L, UDF(struct(feature_0, feature_0#11852, feature_1, feature_1#11853, feature_2, feature_2#11854, feature_3, feature_3#11855, feature_4, feature_4#11856)) AS features#11122]\n         :     :                    +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, monotonically_increasing_id() AS unique_id#11858L]\n         :     :                       +- LogicalRDD [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L], false\n         :     +- Project [unique_id#11880L, prediction#11651 AS prediction_8#11677]\n         :        +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, unique_id#11880L, features#11122, rawPrediction#11621, probability#11634, UDF(rawPrediction#11621) AS prediction#11651]\n         :           +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, unique_id#11880L, features#11122, rawPrediction#11621, UDF(rawPrediction#11621) AS probability#11634]\n         :              +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, unique_id#11880L, features#11122, UDF(features#11122) AS rawPrediction#11621]\n         :                 +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, unique_id#11880L, UDF(struct(feature_0, feature_0#11874, feature_1, feature_1#11875, feature_2, feature_2#11876, feature_3, feature_3#11877, feature_4, feature_4#11878)) AS features#11122]\n         :                    +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, monotonically_increasing_id() AS unique_id#11880L]\n         :                       +- LogicalRDD [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L], false\n         +- Project [unique_id#11903L, prediction#11712 AS prediction_9#11738]\n            +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, unique_id#11903L, features#11122, rawPrediction#11682, probability#11695, UDF(rawPrediction#11682) AS prediction#11712]\n               +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, unique_id#11903L, features#11122, rawPrediction#11682, UDF(rawPrediction#11682) AS probability#11695]\n                  +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, unique_id#11903L, features#11122, UDF(features#11122) AS rawPrediction#11682]\n                     +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, unique_id#11903L, UDF(struct(feature_0, feature_0#11897, feature_1, feature_1#11898, feature_2, feature_2#11899, feature_3, feature_3#11900, feature_4, feature_4#11901)) AS features#11122]\n                        +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, monotonically_increasing_id() AS unique_id#11903L]\n                           +- LogicalRDD [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipeline_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_spark\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/ml/pipeline.py:304\u001b[0m, in \u001b[0;36mPipelineModel._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages:\n\u001b[0;32m--> 304\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "Cell \u001b[0;32mIn[96], line 67\u001b[0m, in \u001b[0;36mBaggingModel._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Compute final prediction (majority vote)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m cols \u001b[38;5;241m=\u001b[39m [F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels))]\n\u001b[0;32m---> 67\u001b[0m final_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpred_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_prediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprediction_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_prediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maggregate(final_prediction, 0, (acc, x) -> acc + x) / \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m > 0.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     69\u001b[0m                           \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Select final output\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_prediction\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   3035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/skope-rules-pyspark/.venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/skope-rules-pyspark/.venv/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'aggregate(final_prediction, 0, lambdafunction((CAST(namedlambdavariable() AS DOUBLE) + namedlambdavariable()), namedlambdavariable(), namedlambdavariable()), lambdafunction(namedlambdavariable(), namedlambdavariable()))' due to data type mismatch: argument 3 requires int type, however, 'lambdafunction((CAST(namedlambdavariable() AS DOUBLE) + namedlambdavariable()), namedlambdavariable(), namedlambdavariable())' is of double type.;\nProject [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677, prediction_9#11738, ((cast(aggregate(final_prediction#11921, 0, lambdafunction((cast(lambda acc#11935 as double) + lambda x#11936), lambda acc#11935, lambda x#11936, false), lambdafunction(lambda id#11937, lambda id#11937, false)) as double) / cast(10 as double)) > cast(0.5 as double)) AS final_prediction#11934]\n+- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677, prediction_9#11738, array(prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677, prediction_9#11738) AS final_prediction#11921]\n   +- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677, prediction_9#11738]\n      +- Join LeftOuter, (unique_id#11113L = unique_id#11903L)\n         :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616, prediction_8#11677]\n         :  +- Join LeftOuter, (unique_id#11113L = unique_id#11880L)\n         :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555, prediction_7#11616]\n         :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11858L)\n         :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494, prediction_6#11555]\n         :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11837L)\n         :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433, prediction_5#11494]\n         :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11817L)\n         :     :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372, prediction_4#11433]\n         :     :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11798L)\n         :     :     :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311, prediction_3#11372]\n         :     :     :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11780L)\n         :     :     :     :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250, prediction_2#11311]\n         :     :     :     :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11763L)\n         :     :     :     :     :     :     :     :- Project [unique_id#11113L, prediction_0#11189, prediction_1#11250]\n         :     :     :     :     :     :     :     :  +- Join LeftOuter, (unique_id#11113L = unique_id#11747L)\n         :     :     :     :     :     :     :     :     :- Project [unique_id#11113L, prediction#11163 AS prediction_0#11189]\n         :     :     :     :     :     :     :     :     :  +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, unique_id#11113L, features#11122, rawPrediction#11133, probability#11146, UDF(rawPrediction#11133) AS prediction#11163]\n         :     :     :     :     :     :     :     :     :     +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, unique_id#11113L, features#11122, rawPrediction#11133, UDF(rawPrediction#11133) AS probability#11146]\n         :     :     :     :     :     :     :     :     :        +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, unique_id#11113L, features#11122, UDF(features#11122) AS rawPrediction#11133]\n         :     :     :     :     :     :     :     :     :           +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, unique_id#11113L, UDF(struct(feature_0, feature_0#184, feature_1, feature_1#185, feature_2, feature_2#186, feature_3, feature_3#187, feature_4, feature_4#188)) AS features#11122]\n         :     :     :     :     :     :     :     :     :              +- Project [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L, monotonically_increasing_id() AS unique_id#11113L]\n         :     :     :     :     :     :     :     :     :                 +- LogicalRDD [feature_0#184, feature_1#185, feature_2#186, feature_3#187, feature_4#188, label#189L], false\n         :     :     :     :     :     :     :     :     +- Project [unique_id#11747L, prediction#11224 AS prediction_1#11250]\n         :     :     :     :     :     :     :     :        +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, unique_id#11747L, features#11122, rawPrediction#11194, probability#11207, UDF(rawPrediction#11194) AS prediction#11224]\n         :     :     :     :     :     :     :     :           +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, unique_id#11747L, features#11122, rawPrediction#11194, UDF(rawPrediction#11194) AS probability#11207]\n         :     :     :     :     :     :     :     :              +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, unique_id#11747L, features#11122, UDF(features#11122) AS rawPrediction#11194]\n         :     :     :     :     :     :     :     :                 +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, unique_id#11747L, UDF(struct(feature_0, feature_0#11741, feature_1, feature_1#11742, feature_2, feature_2#11743, feature_3, feature_3#11744, feature_4, feature_4#11745)) AS features#11122]\n         :     :     :     :     :     :     :     :                    +- Project [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L, monotonically_increasing_id() AS unique_id#11747L]\n         :     :     :     :     :     :     :     :                       +- LogicalRDD [feature_0#11741, feature_1#11742, feature_2#11743, feature_3#11744, feature_4#11745, label#11746L], false\n         :     :     :     :     :     :     :     +- Project [unique_id#11763L, prediction#11285 AS prediction_2#11311]\n         :     :     :     :     :     :     :        +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, unique_id#11763L, features#11122, rawPrediction#11255, probability#11268, UDF(rawPrediction#11255) AS prediction#11285]\n         :     :     :     :     :     :     :           +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, unique_id#11763L, features#11122, rawPrediction#11255, UDF(rawPrediction#11255) AS probability#11268]\n         :     :     :     :     :     :     :              +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, unique_id#11763L, features#11122, UDF(features#11122) AS rawPrediction#11255]\n         :     :     :     :     :     :     :                 +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, unique_id#11763L, UDF(struct(feature_0, feature_0#11757, feature_1, feature_1#11758, feature_2, feature_2#11759, feature_3, feature_3#11760, feature_4, feature_4#11761)) AS features#11122]\n         :     :     :     :     :     :     :                    +- Project [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L, monotonically_increasing_id() AS unique_id#11763L]\n         :     :     :     :     :     :     :                       +- LogicalRDD [feature_0#11757, feature_1#11758, feature_2#11759, feature_3#11760, feature_4#11761, label#11762L], false\n         :     :     :     :     :     :     +- Project [unique_id#11780L, prediction#11346 AS prediction_3#11372]\n         :     :     :     :     :     :        +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, unique_id#11780L, features#11122, rawPrediction#11316, probability#11329, UDF(rawPrediction#11316) AS prediction#11346]\n         :     :     :     :     :     :           +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, unique_id#11780L, features#11122, rawPrediction#11316, UDF(rawPrediction#11316) AS probability#11329]\n         :     :     :     :     :     :              +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, unique_id#11780L, features#11122, UDF(features#11122) AS rawPrediction#11316]\n         :     :     :     :     :     :                 +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, unique_id#11780L, UDF(struct(feature_0, feature_0#11774, feature_1, feature_1#11775, feature_2, feature_2#11776, feature_3, feature_3#11777, feature_4, feature_4#11778)) AS features#11122]\n         :     :     :     :     :     :                    +- Project [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L, monotonically_increasing_id() AS unique_id#11780L]\n         :     :     :     :     :     :                       +- LogicalRDD [feature_0#11774, feature_1#11775, feature_2#11776, feature_3#11777, feature_4#11778, label#11779L], false\n         :     :     :     :     :     +- Project [unique_id#11798L, prediction#11407 AS prediction_4#11433]\n         :     :     :     :     :        +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, unique_id#11798L, features#11122, rawPrediction#11377, probability#11390, UDF(rawPrediction#11377) AS prediction#11407]\n         :     :     :     :     :           +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, unique_id#11798L, features#11122, rawPrediction#11377, UDF(rawPrediction#11377) AS probability#11390]\n         :     :     :     :     :              +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, unique_id#11798L, features#11122, UDF(features#11122) AS rawPrediction#11377]\n         :     :     :     :     :                 +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, unique_id#11798L, UDF(struct(feature_0, feature_0#11792, feature_1, feature_1#11793, feature_2, feature_2#11794, feature_3, feature_3#11795, feature_4, feature_4#11796)) AS features#11122]\n         :     :     :     :     :                    +- Project [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L, monotonically_increasing_id() AS unique_id#11798L]\n         :     :     :     :     :                       +- LogicalRDD [feature_0#11792, feature_1#11793, feature_2#11794, feature_3#11795, feature_4#11796, label#11797L], false\n         :     :     :     :     +- Project [unique_id#11817L, prediction#11468 AS prediction_5#11494]\n         :     :     :     :        +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, unique_id#11817L, features#11122, rawPrediction#11438, probability#11451, UDF(rawPrediction#11438) AS prediction#11468]\n         :     :     :     :           +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, unique_id#11817L, features#11122, rawPrediction#11438, UDF(rawPrediction#11438) AS probability#11451]\n         :     :     :     :              +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, unique_id#11817L, features#11122, UDF(features#11122) AS rawPrediction#11438]\n         :     :     :     :                 +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, unique_id#11817L, UDF(struct(feature_0, feature_0#11811, feature_1, feature_1#11812, feature_2, feature_2#11813, feature_3, feature_3#11814, feature_4, feature_4#11815)) AS features#11122]\n         :     :     :     :                    +- Project [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L, monotonically_increasing_id() AS unique_id#11817L]\n         :     :     :     :                       +- LogicalRDD [feature_0#11811, feature_1#11812, feature_2#11813, feature_3#11814, feature_4#11815, label#11816L], false\n         :     :     :     +- Project [unique_id#11837L, prediction#11529 AS prediction_6#11555]\n         :     :     :        +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, unique_id#11837L, features#11122, rawPrediction#11499, probability#11512, UDF(rawPrediction#11499) AS prediction#11529]\n         :     :     :           +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, unique_id#11837L, features#11122, rawPrediction#11499, UDF(rawPrediction#11499) AS probability#11512]\n         :     :     :              +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, unique_id#11837L, features#11122, UDF(features#11122) AS rawPrediction#11499]\n         :     :     :                 +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, unique_id#11837L, UDF(struct(feature_0, feature_0#11831, feature_1, feature_1#11832, feature_2, feature_2#11833, feature_3, feature_3#11834, feature_4, feature_4#11835)) AS features#11122]\n         :     :     :                    +- Project [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L, monotonically_increasing_id() AS unique_id#11837L]\n         :     :     :                       +- LogicalRDD [feature_0#11831, feature_1#11832, feature_2#11833, feature_3#11834, feature_4#11835, label#11836L], false\n         :     :     +- Project [unique_id#11858L, prediction#11590 AS prediction_7#11616]\n         :     :        +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, unique_id#11858L, features#11122, rawPrediction#11560, probability#11573, UDF(rawPrediction#11560) AS prediction#11590]\n         :     :           +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, unique_id#11858L, features#11122, rawPrediction#11560, UDF(rawPrediction#11560) AS probability#11573]\n         :     :              +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, unique_id#11858L, features#11122, UDF(features#11122) AS rawPrediction#11560]\n         :     :                 +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, unique_id#11858L, UDF(struct(feature_0, feature_0#11852, feature_1, feature_1#11853, feature_2, feature_2#11854, feature_3, feature_3#11855, feature_4, feature_4#11856)) AS features#11122]\n         :     :                    +- Project [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L, monotonically_increasing_id() AS unique_id#11858L]\n         :     :                       +- LogicalRDD [feature_0#11852, feature_1#11853, feature_2#11854, feature_3#11855, feature_4#11856, label#11857L], false\n         :     +- Project [unique_id#11880L, prediction#11651 AS prediction_8#11677]\n         :        +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, unique_id#11880L, features#11122, rawPrediction#11621, probability#11634, UDF(rawPrediction#11621) AS prediction#11651]\n         :           +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, unique_id#11880L, features#11122, rawPrediction#11621, UDF(rawPrediction#11621) AS probability#11634]\n         :              +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, unique_id#11880L, features#11122, UDF(features#11122) AS rawPrediction#11621]\n         :                 +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, unique_id#11880L, UDF(struct(feature_0, feature_0#11874, feature_1, feature_1#11875, feature_2, feature_2#11876, feature_3, feature_3#11877, feature_4, feature_4#11878)) AS features#11122]\n         :                    +- Project [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L, monotonically_increasing_id() AS unique_id#11880L]\n         :                       +- LogicalRDD [feature_0#11874, feature_1#11875, feature_2#11876, feature_3#11877, feature_4#11878, label#11879L], false\n         +- Project [unique_id#11903L, prediction#11712 AS prediction_9#11738]\n            +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, unique_id#11903L, features#11122, rawPrediction#11682, probability#11695, UDF(rawPrediction#11682) AS prediction#11712]\n               +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, unique_id#11903L, features#11122, rawPrediction#11682, UDF(rawPrediction#11682) AS probability#11695]\n                  +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, unique_id#11903L, features#11122, UDF(features#11122) AS rawPrediction#11682]\n                     +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, unique_id#11903L, UDF(struct(feature_0, feature_0#11897, feature_1, feature_1#11898, feature_2, feature_2#11899, feature_3, feature_3#11900, feature_4, feature_4#11901)) AS features#11122]\n                        +- Project [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L, monotonically_increasing_id() AS unique_id#11903L]\n                           +- LogicalRDD [feature_0#11897, feature_1#11898, feature_2#11899, feature_3#11900, feature_4#11901, label#11902L], false\n"
     ]
    }
   ],
   "source": [
    "pipeline_model.transform(df_spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
